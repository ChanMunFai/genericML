{
  
    
        "post0": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://munfai15.github.io/genericML/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://munfai15.github.io/genericML/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Generic Machine Learning",
            "content": "This notebook provides code to Generic Machine Learning Inference on Heterogenous Treatment Effects in Randomized Experiments by Victor Chernozhukov, Mert Demirer, Esther Duflo, and Iván Fernández-Val. . https://arxiv.org/abs/1712.04802 . References . https://github.com/arnaudfrn/MLheterogeneity/blob/dev/src/vb_heterogeneity_FE.R . Many thanks to Arnaud Fournier, who provided the R code for this. . Author of notebook : Mun Fai Chan . Future developments for code . Hyperparemeter tuning on ML estimators | Converting pandas dataframes to LaTex tables. | Aesthetic updates - includes adding astericks for significance | Add in fixed effects | Other developments . Empirical application | Monte Carlo simulation to test veracity and robustness of code | from propscore import PropensityScore import random import pandas as pd import sklearn import sklearn.model_selection import numpy as np import statistics as stats import statsmodels.api as sm from scipy.stats import norm import warnings warnings.filterwarnings(&quot;ignore&quot;, category=FutureWarning) . from causalinference import CausalModel from sklearn.linear_model import LinearRegression from sklearn.preprocessing import OneHotEncoder from sklearn.ensemble import RandomForestRegressor from sklearn import svm from sklearn import datasets, ensemble from sklearn.neural_network import MLPRegressor from sklearn.linear_model import ElasticNet . Data . df = pd.read_csv(&quot;~/OneDrive - London School of Economics/LSE/Year 3/EC331/November/simdata1.csv&quot;) # In this simulated dataset, all controls are uniformly random around (-1,1). Treatment (binary) is randomly assigned # and has a treatment effect of 2.0 + some gaussian noise. controls = [&#39;X1&#39;,&#39;X2&#39;,&#39;X3&#39;,&#39;X4&#39;,&#39;X5&#39;] treatment = &#39;treatment&#39; . ps = PropensityScore(treatment, controls, df); df = df.join(ps.propscore) df.head() . Logit Regression Results ============================================================================== Dep. Variable: treatment No. Observations: 2000 Model: Logit Df Residuals: 1996 Method: MLE Df Model: 3 Date: Mon, 04 Jan 2021 Pseudo R-squ.: 0.003082 Time: 14:12:22 Log-Likelihood: -1382.0 converged: True LL-Null: -1386.3 Covariance Type: nonrobust LLR p-value: 0.03598 ============================================================================== coef std err z P&gt;|z| [0.025 0.975] X4 0.0656 0.044 1.481 0.138 -0.021 0.152 X1XX2 -0.0862 0.046 -1.889 0.059 -0.176 0.003 X4_sq 0.0576 0.032 1.802 0.072 -0.005 0.120 _cons -0.0592 0.056 -1.066 0.286 -0.168 0.050 ============================================================================== The following vars were infeasible: Stratification produced 4 strata . X1 X2 X3 X4 X5 treatment outcome propscore . 0 | 0.222325 | 1.096420 | 0.791501 | -0.047723 | -0.638967 | 1 | 1.896553 | 0.479204 | . 1 | -0.333924 | -2.472571 | -0.635518 | 0.640664 | 0.408980 | 0 | -0.005217 | 0.483821 | . 2 | -0.635914 | -0.714505 | 0.170289 | -0.591279 | -0.211143 | 0 | 0.097441 | 0.470775 | . 3 | 0.618626 | -1.055120 | 0.486168 | -0.441626 | -2.865479 | 0 | 0.015330 | 0.494820 | . 4 | 0.807180 | 1.475266 | -0.033577 | 0.354557 | 0.868843 | 1 | 1.893319 | 0.467209 | . Initialisation . iterations = 100 k = 5 # number of groups for heterogeneity analysis alpha = 0.05 # significance level . Running everything . ML_models = [&quot;random_forest&quot;, &quot;SVM&quot;, &quot;gradient_boost&quot;, &quot;neural_net&quot;, &quot;ElasticNet&quot;] for x in ML_models: summary = Generic_ML_single(df, controls, 10, x, alpha , 5) print (str(x) + &quot;: lamda1: &quot; + str(summary[-2])+ &quot; lambda2: &quot; + str(summary[-1])) . random_forest: lamda1: -5.765410377113008e-05 lambda2: 0.7962556067959392 SVM: lamda1: 6.37687223615527e-05 lambda2: 0.7990199258698686 gradient_boost: lamda1: -5.810927679997994e-06 lambda2: 0.7972126884049128 neural_net: lamda1: -0.00013872698846416478 lambda2: 0.7969536495255202 ElasticNet: lamda1: 0.00038697229802813827 lambda2: 0.7977092031809006 . This allows us to quickly compare between different ML estimators. In particular, we want to minimise lambda1 and lambda2. . summary = Generic_ML_single(df, controls, iterations, &quot;random_forest&quot;, alpha , 5) . BLP = summary[0]; BLP . ATE HET . coeff | 2.002647 | 0.019719 | . se | 0.007427 | 0.074006 | . pvalue | 0.000000 | 0.987714 | . lower bound | 1.987861 | -0.120732 | . upper bound | 2.017433 | 0.162308 | . GATES = summary[1]; GATES . G1 G5 G1 - G5 . coeff | 1.995021 | 2.010402 | -0.012300 | . se | 0.017296 | 0.017322 | 0.024500 | . pvalue | 0.000000 | 0.000000 | 1.000000 | . lower bound | 1.960978 | 1.976616 | -0.061299 | . upper bound | 2.029177 | 2.044189 | 0.036699 | . CLAN = summary[2]; CLAN . coeff se pvalue lower bound upper bound . Most affected X1 | -0.046367 | 0.079034 | 0.393447 | -0.202160 | 0.109426 | . Least affected X1 | -0.025723 | 0.079034 | 0.456220 | -0.181875 | 0.130428 | . Most - least affected X1 | 0.033200 | 0.250000 | 0.693000 | -0.460711 | 0.527111 | . Most affected X2 | 0.026326 | 0.083156 | 0.602332 | -0.136445 | 0.189097 | . Least affected X2 | -0.007814 | 0.083156 | 0.345540 | -0.170585 | 0.154957 | . Most - least affected X2 | 0.034150 | 0.131500 | 0.620500 | -0.222605 | 0.290905 | . Most affected X3 | 0.051797 | 0.081971 | 0.473087 | -0.112999 | 0.216593 | . Least affected X3 | -0.010720 | 0.081971 | 0.637616 | -0.172631 | 0.151190 | . Most - least affected X3 | 0.049050 | 0.130000 | 0.542000 | -0.207705 | 0.305805 | . Most affected X4 | 0.015797 | 0.083432 | 0.578876 | -0.147815 | 0.179409 | . Least affected X4 | -0.031468 | 0.083432 | 0.494258 | -0.197107 | 0.134171 | . Most - least affected X4 | 0.039350 | 0.132000 | 0.532000 | -0.213485 | 0.292185 | . Most affected X5 | -0.008161 | 0.080664 | 0.324056 | -0.166095 | 0.149773 | . Least affected X5 | -0.012703 | 0.080664 | 0.338504 | -0.171438 | 0.146032 | . Most - least affected X5 | 0.010450 | 0.127500 | 0.414000 | -0.240425 | 0.261325 | . HELPER FUNCTIONS . BLP . def BLP(df, alpha): &#39;&#39;&#39; Returns summary results, whose parameters can be used to obtain BLP of CATE. Contains: Estimator Coefficients of Term 2 and 3 Standard Error p values Confidence Interval (lower and upper bounds) Returns lambda1 - value to help choose the best ML method &#39;&#39;&#39; term2 = df[&#39;treatment&#39;] - df[&#39;propscore&#39;] S = df[&#39;S&#39;] term3 = term2 * (S - np.mean(S)) combined = df.copy() combined.loc[:,&#39;term2&#39;] = term2 combined.loc[:,&#39;term3&#39;] = term3 combined.loc[:,&#39;ones&#39;] = 1 X_reg = combined[[&#39;B&#39;, &#39;S&#39;, &#39;ones&#39;, &#39;term2&#39;, &#39;term3&#39;]] y = combined[[&#39;outcome&#39;]] regBLP = sm.OLS(y, X_reg) res_BLP = regBLP.fit() res_BLP = results_summary_to_dataframe(res_BLP, alpha) lambda1 = res_BLP.iloc[-1,0] * stats.variance(S) return res_BLP, lambda1 def results_summary_to_dataframe(results, alpha): &#39;&#39;&#39;take the result of an statsmodel results table and transforms it into a dataframe&#39;&#39;&#39; pvals = results.pvalues coeff = results.params std_err = results.bse crit_val = norm.ppf(1-alpha/2) lb = coeff - std_err * crit_val ub = coeff + std_err * crit_val results_df = pd.DataFrame({&quot;pvals&quot;:pvals, &quot;coeff&quot;:coeff, &quot;lb&quot;:lb, &quot;ub&quot;:ub, &quot;std_err&quot;:std_err, }) #Reordering... results_df = results_df[[&quot;coeff&quot;,&quot;std_err&quot;,&quot;pvals&quot;,&quot;lb&quot;,&quot;ub&quot;]] return results_df def BLP_to_storage(res_BLP): &#39;&#39;&#39; Takes the output of BLP and store them as lists, whereby the output refers to: res_BLP - summary table containing parameters to construct BLP, along with their p-values, standard errors and lower and upper bounds Returns 2 lists data_HET and data_ATE whose array-equivalent is of dimension (1 variable, 5 attributes) &#39;&#39;&#39; # HET parameter HET = res_BLP.iloc[-1,0] HET_se = res_BLP.iloc[-1,1] HET_pvals = res_BLP.iloc[-1, 2] HET_lb = res_BLP.iloc[-1, 3] HET_ub = res_BLP.iloc[-1, 4] # ATE ATE = res_BLP.iloc[-2,0] ATE_se = res_BLP.iloc[-2,1] ATE_pvals = res_BLP.iloc[-2,2] ATE_lb = res_BLP.iloc[-2,3] ATE_ub = res_BLP.iloc[-2,4] # Storage data_HET = [HET, HET_se, HET_pvals, HET_lb, HET_ub] data_ATE = [ATE, ATE_se, ATE_pvals, ATE_lb, ATE_ub] return data_HET, data_ATE . GATES . def GATES(df, k , alpha): &#39;&#39;&#39; Returns summary statistics, whose results can give us the average treatment effect for most and least affected group. Contains: Estimator Coefficients Standard Error p values Confidence Interval (lower and upper bounds) Returns lambda2 - value to help choose the best ML method Parameters - df -- (main) dataframe which must contain the following items: propensity score B - proxy predictor for BCA S - proxy predictor for CATE treatment k -- number of groups &#39;&#39;&#39; combined = df.copy() term2 = df[&#39;treatment&#39;] - df[&#39;propscore&#39;] combined.loc[:,&#39;term2&#39;] = term2 combined.loc[:,&#39;ones&#39;] = 1 groups = groups_multiply(df, group_create(k, df), k) combined = pd.concat([combined,groups], axis = 1) controls = [&quot;B&quot;, &quot;S&quot;, &quot;ones&quot;] + [&quot;G&quot; + str(i) for i in range(1,k+1)] X_GATES = combined[controls] # modify for auto selection of columns y = combined[[&#39;outcome&#39;]] regGATES = sm.OLS(y, X_GATES) res_GATES = regGATES.fit() # Hypothesis testing hypothesis = &quot;(G1 = &quot; + &quot;G&quot; + str(k) + &quot;)&quot; # G1 = G{k} t_test_html = res_GATES.t_test(hypothesis).summary().as_html() t_test = pd.read_html(t_test_html, header=0, index_col=0)[0] res_GATES = results_summary_to_dataframe(res_GATES, alpha) lambda2 = res_GATES.iloc[3:, 0].mean()**2 / k return res_GATES, t_test, lambda2 def group_create(k, df): &#39;&#39;&#39; Returns quantiles of the variable &#39;S&#39;, encoded into dummy variables &#39;&#39;&#39; breaks = df[&#39;S&#39;].quantile(np.linspace(0,1,(k+1))) breaks.iloc[0,] = breaks.iloc[0,] - 0.001 breaks.iloc[k,] = breaks.iloc[k,] - 0.001 combined = df.copy() combined[&#39;Groups&#39;] = pd.cut(x= df[&#39;S&#39;], bins = breaks) # this will fail if there are too many groups groups = pd.get_dummies(combined[&#39;Groups&#39;]) return groups def groups_multiply(df, groups, k): &#39;&#39;&#39; Multiply groups dataframe with term 2 and rename columns &#39;&#39;&#39; combined = df.copy() term2 = df[&#39;treatment&#39;] - df[&#39;propscore&#39;] combined.loc[:,&#39;term2&#39;] = term2 groups = np.multiply(groups, combined[&#39;term2&#39;].values.reshape(len(df.index),1)) groups.columns = [&quot;G&quot; + str(i) for i in range(1,k+1)] return groups def GATES_to_storage(res_GATES, t_test_GATES, alpha): &#39;&#39;&#39; Takes the output of GATES and store them as lists, whereby the output refers to: res_GATES - summary table containing parameters to construct GATES, along with their p-values and standard errors t_test_GATEs - t test table to determine if G1 = Gk Returns a list whose array-equivalent is dimension of (# of variables, # of attributes ) &#39;&#39;&#39; # Most affected group gamma1 = res_GATES.iloc[3,0] gamma1_se = res_GATES.iloc[3,1] gamma1_pvals = res_GATES.iloc[3,2] gamma1_lb = res_GATES.iloc[3,3] gamma1_ub = res_GATES.iloc[3,4] # Least affected group gammak = res_GATES.iloc[-1,0] gammak_se = res_GATES.iloc[-1,1] gammak_pvals = res_GATES.iloc[-1,2] gammak_lb = res_GATES.iloc[-1,3] gammak_ub = res_GATES.iloc[-1,4] # Difference between most and least affected group crit_val = norm.ppf(1-alpha/2) gamma_diff = t_test_GATES.iloc[0,0] gamma_diff_se = t_test_GATES.iloc[0,1] gamma_diff_pvals = t_test_GATES.iloc[0,3] gamma_diff_lb = gamma_diff - crit_val * gamma_diff_se gamma_diff_ub = gamma_diff + crit_val * gamma_diff_se data_gamma1 = [gamma1, gamma1_se, gamma1_pvals, gamma1_lb, gamma1_ub] data_gammak = [gammak, gammak_se, gammak_pvals, gammak_lb, gammak_ub] data_gamma_diff = [gamma_diff, gamma_diff_se, gamma_diff_pvals, gamma_diff_lb, gamma_diff_ub] data_gamma = [data_gamma1, data_gammak, data_gamma_diff] return data_gamma . CLAN . def CLAN(df, controls, k = 5, alpha): data_CLAN_loop = [] for x in controls: res_CLAN, t_test = CLAN_single(df, x, k) data_CLAN = CLAN_to_storage(res_CLAN, t_test, alpha) data_CLAN_loop.append(data_CLAN) return data_CLAN_loop def CLAN_single(df, control, k = 5): &#39;&#39;&#39; Returns the average characteristic for one control between the most and least affected groups &#39;&#39;&#39; threshold = 1/k high_effect = df[&#39;S&#39;].quantile(1 - threshold) low_effect = df[&#39;S&#39;].quantile(threshold) combined = df.copy() combined.loc[:,&#39;high&#39;] = (combined.loc[:,&quot;S&quot;] &gt; high_effect).astype(int) # dummy variables for high combined.loc[:,&#39;low&#39;] = (combined.loc[:,&quot;S&quot;] &gt; low_effect).astype(int) # dummy variables for low combined.loc[:,&#39;minusones&#39;] = -1 X_control = combined[[&#39;high&#39;, &#39;low&#39;, &#39;minusones&#39;]] # I have no idea why I included minusones y_control = combined[[control]] reg_CLAN = sm.OLS(y_control, X_control) res_CLAN = reg_CLAN.fit() hypothesis = &quot;(high = low)&quot; t_test_html = res_CLAN.t_test(hypothesis).summary().as_html() t_test = pd.read_html(t_test_html, header=0, index_col=0)[0] res_CLAN = results_summary_to_dataframe(res_CLAN, alpha) return res_CLAN, t_test def CLAN_to_storage(res_CLAN, t_test, alpha): &#39;&#39;&#39; Takes the summary results of CLAN and its t test and store them as lists &#39;&#39;&#39; h_coeff = res_CLAN.iloc[0,0] h_se = res_CLAN.iloc[0,1] h_pvals = res_CLAN.iloc[0,2] h_lb = res_CLAN.iloc[0,3] h_ub = res_CLAN.iloc[0,4] data_h = [h_coeff, h_se, h_pvals, h_lb, h_ub] l_coeff = res_CLAN.iloc[1,0] l_se = res_CLAN.iloc[1,1] l_pvals = res_CLAN.iloc[1,2] l_lb = res_CLAN.iloc[1,3] l_ub = res_CLAN.iloc[1,4] data_l = [l_coeff, l_se, l_pvals, l_lb, l_ub] crit_val = norm.ppf(1-alpha/2) diff_coeff = t_test.iloc[0,0] diff_se = t_test.iloc[0,1] diff_pvals = t_test.iloc[0,3] diff_lb = diff_coeff - crit_val * diff_se diff_ub = diff_coeff + crit_val * diff_se data_diff = [diff_coeff, diff_se, diff_pvals, diff_lb, diff_ub] data_CLAN = data_h, data_l, data_diff return data_CLAN . Converting data into dataframes . def data_BLP_to_df(data_HET_loop, data_ATE_loop): &#39;&#39;&#39; Takes the data of BLP stored as a list, find its median over different iterations and adjusts p values. Returns it as a dataframe &#39;&#39;&#39; data_HET_array = np.array(data_HET_loop) data_HET_final = np.median(data_HET_array, axis = 0) data_HET_final[2] = np.minimum(1, data_HET_final[2] *2) data_ATE_array = np.array(data_ATE_loop) data_ATE_final = np.median(data_ATE_array, axis = 0) data_ATE_final[2] = np.minimum(1, data_ATE_final[2] * 2) df_ATE = pd.DataFrame(data_ATE_final, index = [&#39;coeff&#39;, &#39;se&#39;, &#39;pvalue&#39;, &#39;lower bound&#39;, &#39;upper bound&#39;], columns = [&#39;ATE&#39;]) df_HET = pd.DataFrame(data_HET_final, index = [&#39;coeff&#39;, &#39;se&#39;, &#39;pvalue&#39;, &#39;lower bound&#39;, &#39;upper bound&#39;], columns = [&#39;HET&#39;]) frames = [df_ATE, df_HET] df_BLP = pd.concat(frames, axis = 1) return df_BLP def data_GATES_to_df(data_GATES_loop, groups): &#39;&#39;&#39; Takes the data of GATES stored as a list, find its median over different iterations and adjusts p values. Returns it as a dataframe &#39;&#39;&#39; # GATES data_GATES_array = np.array(data_GATES_loop) data_GATES_final = np.median(data_GATES_array, axis = 0) data_GATES_final[:, 2] = np.minimum(1, data_GATES_final[:, 2]* 2) df_GATES = pd.DataFrame(data_GATES_final, columns = [&#39;coeff&#39;, &#39;se&#39;, &#39;pvalue&#39;, &#39;lower bound&#39;, &#39;upper bound&#39;], index = [&#39;G1&#39;, &quot;G&quot; + str(groups), &quot;G1 - G&quot; + str(groups)]) return df_GATES.transpose() def data_CLAN_to_df(data_CLAN_loop, controls = controls): &#39;&#39;&#39; Takes the data of GATES stored as a list, find its median over different iterations and adjusts p values. Returns it as a dataframe &#39;&#39;&#39; # CLAN data_CLAN_array = np.array(data_CLAN_loop) data_CLAN_final = np.median(data_CLAN_array, axis = 0) # This code is technically wrong as we take the upper medians for the lower bounds data_CLAN_final[0,2,:] = np.minimum(1, data_CLAN_final[0,2,:] * 2) list = [] for x in controls: list1 = [&#39;Most affected &#39; + str(x), &#39;Least affected &#39; + str(x), &#39;Most - least affected &#39; + str(x) ] list.append(list1) flattened_list = [y for x in list for y in x] data_CLAN_new = data_CLAN_final.reshape(-1,5) df_CLAN = pd.DataFrame(data_CLAN_new, columns = [&#39;coeff&#39;, &#39;se&#39;, &#39;pvalue&#39;, &#39;lower bound&#39;, &#39;upper bound&#39;], index = flattened_list) return df_CLAN . Putting everything together . def Generic_ML_single(df, controls, iterations = 10, model = &quot;random_forest&quot;, alpha = 0.05, k = 5): &#39;&#39;&#39; Runs the whole generic ML algorithm for a ML model and returns a list of datasets for all parameters. &#39;&#39;&#39; data_HET_loop = [] data_ATE_loop = [] lambda1_loop = [] data_GATES_loop = [] lambda2_loop = [] data_CLAN_loop = [] for x in range(iterations): main, aux = sklearn.model_selection.train_test_split(df, train_size = 0.5, random_state = x) main2 = ML_estimator(main, aux, model) # BLP res_BLP, lambda1 = BLP(main2) data_HET, data_ATE = BLP_to_storage(res_BLP) data_HET_loop.append(data_HET) data_ATE_loop.append(data_ATE) lambda1_loop.append(lambda1) #GATES res_GATES, t_test_GATES, lambda2 = GATES(main2, k, alpha) data_GATES = GATES_to_storage(res_GATES, t_test_GATES, alpha) data_GATES_loop.append(data_GATES) lambda2_loop.append(lambda2) # CLAN controls = controls data_CLAN = CLAN(main2, controls) data_CLAN_loop.append(data_CLAN) # BLP data_HET_array = np.array(data_HET_loop) data_HET_final = np.median(data_HET_array, axis = 0) data_HET_final[2] = np.minimum(1, data_HET_final[2] *2) data_ATE_array = np.array(data_ATE_loop) data_ATE_final = np.median(data_ATE_array, axis = 0) data_ATE_final[2] = np.minimum(1, data_ATE_final[2] * 2) df_BLP = data_BLP_to_df(data_HET_loop, data_ATE_loop) df_GATES = data_GATES_to_df(data_GATES_loop, k) df_CLAN = data_CLAN_to_df(data_CLAN_loop, controls = controls) lambda1 = np.mean(lambda1_loop) lamda2 = np.mean(lambda2_loop) summary = [df_BLP, df_GATES, df_CLAN, lambda1, lambda2] return summary . ML estimators . def ML_estimator(main, aux, model): &#39;&#39;&#39; Returns the main dataset combined with B and S, which are proxy predictors for BCA and CATE respectively Parameters - main: main dataset which must contain treatment and outcome aux: auxilliary dataset which must contain treatment and outcome model - in string format models = [&quot;random_forest&quot;, &quot;SVM&quot;, &quot;gradient_boost&quot;, &quot;neural_net&quot;, &quot;ElasticNet&quot;] # need to set the seed of the ML_estimators &#39;&#39;&#39; # Initialization aux0 = aux[aux[&#39;treatment&#39;] == 0] aux1 = aux[aux[&#39;treatment&#39;] == 1] X_aux0 = aux0[[&#39;treatment&#39;, &#39;X1&#39;, &#39;X2&#39;, &#39;X3&#39;, &#39;X4&#39;, &#39;X5&#39;]] y_aux0 =aux0[&#39;outcome&#39;] X_aux1 = aux1[[&#39;treatment&#39;, &#39;X1&#39;, &#39;X2&#39;, &#39;X3&#39;, &#39;X4&#39;, &#39;X5&#39;]] y_aux1 =aux1[&#39;outcome&#39;] X_main = main[[&#39;treatment&#39;, &#39;X1&#39;, &#39;X2&#39;, &#39;X3&#39;, &#39;X4&#39;, &#39;X5&#39;]] y_main = main[&#39;outcome&#39;] # Model if model == &quot;random_forest&quot;: combined = random_forest(main, X_aux0, y_aux0, X_main, X_aux1, y_aux1) elif model == &quot;SVM&quot;: combined = SVM(main, X_aux0, y_aux0, X_main, X_aux1, y_aux1) elif model == &quot;gradient_boost&quot;: combined = gradient_boost(main, X_aux0, y_aux0, X_main, X_aux1, y_aux1) elif model == &quot;neural_net&quot;: combined = neural_net(main, X_aux0, y_aux0, X_main, X_aux1, y_aux1) elif model == &quot;ElasticNet&quot;: combined = ElasticNet(main, X_aux0, y_aux0, X_main, X_aux1, y_aux1) # Add variance if stats.variance(combined[&#39;S&#39;]) == 0 : combined[&#39;S&#39;] = combined[&#39;S&#39;] + np.random.normal(0,0.1, len(combined[&#39;S&#39;])) if stats.variance(combined[&#39;B&#39;]) == 0 : combined[&#39;B&#39;] = combined[&#39;B&#39;] + np.random.normal(0,0.1, len(combined[&#39;B&#39;])) return combined def random_forest(main, X_aux0, y_aux0, X_main, X_aux1, y_aux1): # Model clf = RandomForestRegressor(max_depth=2, random_state=0) clf.fit(X_aux0, y_aux0) B = clf.predict(X_main) clf.fit(X_aux1, y_aux1) clf.predict(X_main) S = clf.predict(X_main) - B combined = main.copy() combined[&#39;B&#39;] = B combined[&#39;S&#39;] = S return combined def SVM(main, X_aux0, y_aux0, X_main, X_aux1, y_aux1): # Model clf = svm.SVR() clf.fit(X_aux0, y_aux0) B = clf.predict(X_main) clf.fit(X_aux1, y_aux1) clf.predict(X_main) S = clf.predict(X_main) - B combined = main.copy() combined[&#39;B&#39;] = B combined[&#39;S&#39;] = S return combined def gradient_boost(main, X_aux0, y_aux0, X_main, X_aux1, y_aux1): params = {&#39;n_estimators&#39;: 500, &#39;max_depth&#39;: 4, &#39;min_samples_split&#39;: 5, &#39;learning_rate&#39;: 0.01, &#39;loss&#39;: &#39;ls&#39;} # Model clf = ensemble.GradientBoostingRegressor(**params) clf.fit(X_aux0, y_aux0) B = clf.predict(X_main) clf.fit(X_aux1, y_aux1) clf.predict(X_main) S = clf.predict(X_main) - B combined = main.copy() combined[&#39;B&#39;] = B combined[&#39;S&#39;] = S return combined def neural_net(main, X_aux0, y_aux0, X_main, X_aux1, y_aux1): # Model clf = MLPRegressor(solver=&#39;lbfgs&#39;, alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1) clf.fit(X_aux0, y_aux0) B = clf.predict(X_main) clf.fit(X_aux1, y_aux1) clf.predict(X_main) S = clf.predict(X_main) - B combined = main.copy() combined[&#39;B&#39;] = B combined[&#39;S&#39;] = S return combined def ElasticNet(main, X_aux0, y_aux0, X_main, X_aux1, y_aux1): # Model clf = sklearn.linear_model.ElasticNet() clf.fit(X_aux0, y_aux0) B = clf.predict(X_main) clf.fit(X_aux1, y_aux1) clf.predict(X_main) S = clf.predict(X_main) - B combined = main.copy() combined[&#39;B&#39;] = B combined[&#39;S&#39;] = S return combined .",
            "url": "https://munfai15.github.io/genericML/fastpages/jupyter/2020/01/04/Generic-ML.html",
            "relUrl": "/fastpages/jupyter/2020/01/04/Generic-ML.html",
            "date": " • Jan 4, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://munfai15.github.io/genericML/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://munfai15.github.io/genericML/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}