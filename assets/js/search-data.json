{
  
    
        "post0": {
            "title": "Comparative Metrics Framework",
            "content": "Comparative Metrics Framework . This abridged document[1] presents a comprehensive comparative metrics framework to evaluate the performance of synthetic data based on general utility, specific utility and disclosure risk. This is one of the few publicly available benchmarks to evaluate the quality of synthetic data. I will also provide some recommendations on best practices for generating synthetic data. . Introduction . I introduce a comparative metrics framework heavily inspired by Taub et. al (2019). Rather than reinventing the wheel, my focus here is to provide a practical guide to its implementation. In the event where certain code were not previously publicly available, I uploaded them into an R package cmf [2]. . The functions used here are either from the cmf package or the synthpop package, which is a package to synthesise data. However, these functions can be generalised to any synthetic data generated with any method. . General Utility . General utility is a measure of the similarity in distributions between synthetic and original data. A visual examination of histograms or frequency tables will be useful as a preliminary check, but becomes too infeasible when comparing between a large number of synthetic datasets. Instead, a single summary statistic score is necessary for easy comparison of performance. . Ratio of Counts(ROC) . This evaluates the overlap in frequency counts between the original and synthetic dataset. I coded the mathematical equations introduced by Taub et el. (2020) in the cmf package. . [ frac{ min(y_{o}, y_{s}) } {max(y_{o}, y_{s}) }] . (y_{o}) denotes the frequency count (by proportion) for a particular variable of the original dataset. (y_{s}) denotes the same for a synthetic dataset. Intuitively, this is analogous to the intersect of the 2 datasets divided by their union. The ROC score is bounded by 1 and 0. The lower the ROC score, the more distant the 2 datasets are in distribution. The function ROC_score evaluates the ROC score for all individual columns and averages them in the final output. . ROC_numeric evaluates the ROC score for numerical variables, because continuous numerical values in the original and synthetic data may not match exactly. For instance, 10,000 may be arbitrarily synthesised as 10,001 in the synthetic dataset but one may choose to treat them as the same value. Hence, ROC_numeric rounds numeric values off to a user-specified value. . Propensity Scores . Intuitively, propensity score is the probability of a row of data being synthetic given its entries. One can select the column(s) to calculate the propensity score on. A higher ratio to degree of freedom score obtained from synthpop::utility.tab indicates a worse general utility. Ratios bigger than 3 or 4 suggests that a variable is problematic and may warrant further attention - visual examination of these columns is useful. . Specific Utility . Specific utility evaluates if we get similar estimates when running the same statistical analysis on both synthetic and original datasets. . Confidence Interval Overlap (CIO) . Here, I focus on logistic and linear regressions, though we can easily extend this to other machine learning inferences. I make use of the synthpop::lm.synds() and synthpop::glm.synds() functions to run regressions(linear and logistic respectively) on the original and synthetic dataset and evaluate how close confidence intervals are. . Disclosure Risk . Differential Correct Attribution Probabibility (DCAP) . I coded functions for DCAP (Taub et al, 2018) in the R package cmf. DCAP works on the assumption that an intruder has certain key information about individuals. Such key information are highly observable or obtainable - for instance a person’s gender. Using these key information, the intruder attempts to extract target information that are more private - for instance a person’s income level. We define (d_{o}) as the original data and (K_{o}) and (T_{o}) as vectors for the key and target variables. . [d_{o} = {K_{o} , T_{o}}] . Similarly, (d_{s}) is the synthetic data. . [d_{s} = {K_{s} , T_{s}}] . The Correct Attribution Probability (CAP) for the record (j) is the probability of its target variables given its key variables. This is coded as the function CAP_original . We can think of the CAP score for the original dataset to be an approximate upper bound - it would not make much sense for any other dataset to reveal more information than the original dataset. . [CAP_{o,j} = Pr(T_{o,j} | K_{o,j}) = frac{ sum limits_{i=1}^n [T_{o,i} = T_{o,j} , K_{o,i} = K_{o,j}] } { sum limits_{i=1}^n [ K_{o,i} = K_{o,j}]}] . where the square brackets are Iverson brackets and (n) is the number of records. . The CAP for the record (j) based on a corresponding synthetic dataset (d_{s}) is the same probability but derived from (d_{s}). This has been coded as CAP_synthetic. . [CAP_{s,j} = Pr(T_{o,j} | K_{o,j}){s} = frac{ sum limits{i=1}^n [T_{s,i} = T_{o,j} , K_{s,i} = K_{o,j}] } { sum limits_{i=1}^n [ K_{s,i} = K_{o,j}]}] . For any record in the original dataset for which there is no corresponding record in the synthetic dataset with the same key variables, the denominator in Equation (5) will be 0 and the CAP is therefore undefined. We can record non-matches as zero or treat them as undefined; the record is skipped over and does not count towards (n). . To calculate the overall CAP for a synthetic dataset, we divide its CAP score by its corresponding upper bound - the CAP score for the original dataset. This process is repeated for all combinations of key and target variables. . Provisional Findings and Recommendations . I then apply the framework to analyse 3 different datasets. However, due to their sensitivity, I cannot reference them publicly and can only provide provisional recommendations for best practices. . Rules should be the foremost consideration in data synthesis. . When relationships between variables are violated in the synthetic data, they lead to patently spurious results. For instance, if all individuals below (16) should be single, we have to check that this relationship holds in the synthetic datataset. Synthpop allows for the assertion of rules during the synthesis process which addresses this problem. . Visit sequences do not appear to have a significant and consistent impact on performance. . A visit sequence is the order in which variables are synthesised. Visit sequences matter to the performance of a synthetic data only insofar as multiple visit sequences generate random variations which increase the likelihood of finding a ‘better’ dataset. There appears to be no significant and consistent difference in performance for visit sequences generated by different mechanisms. . A quick method for generating different visit sequences is to use the varrank package in R to plot feature importance for each dependent variable. This is significantly faster than using Random Forests to find feature importance. A heuristic is to place the most important variables at the front of the visit sequence. . Speeding Up the Synthesis Process . When experimenting with a new code, it is desirable to synthesise a small sub-sample of the dataset if the synthesis time is long. Use the synthpop::compare() function to do a preliminary examination to ensure that the code is working. Errors may only appear after the synthesis has proceeded for some time. Restarting R during synthesis will cause all local variables to be lost. . | For variables with many categories, one can combine less frequent categories as an ‘Others’ category. This reduces computational time during synthesis and is useful for removing outliers who face a higher disclosure risk. On the other hand, one may risk worsening general and specific utility. . | When working with (DateTime) formats, computational time is significantly improved if these are changed to numeric variables. One may also remove the days (and months) depending on the level of precision required. This improves computational time slightly. . | When working with factors, ensure that factor levels are coded as their original names(for instance as strings) rather than as integers. This seemingly trivial difference may cause hours of difference in synthesis time. . | [1] Original document available at https://github.com/MUNFAI15/DiffPriv . [2] Available for download in R using devtools::install(_)github(“MUNFAI15/cmf”) .",
            "url": "https://munfai15.github.io/genericML/markdown/2020/12/31/Comparative-Metrics-Framework.html",
            "relUrl": "/markdown/2020/12/31/Comparative-Metrics-Framework.html",
            "date": " • Dec 31, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Generic Machine Learning",
            "content": "This notebook provides code to Generic Machine Learning Inference on Heterogenous Treatment Effects in Randomized Experiments by Victor Chernozhukov, Mert Demirer, Esther Duflo, and Iván Fernández-Val. . https://arxiv.org/abs/1712.04802 . References . https://github.com/arnaudfrn/MLheterogeneity/blob/dev/src/vb_heterogeneity_FE.R . Many thanks to Arnaud Fournier, who provided the R code for this. . Author of notebook : Mun Fai Chan . Future developments for code . Hyperparemeter tuning on ML estimators | Converting pandas dataframes to LaTex tables. | Aesthetic updates - includes adding astericks for significance | Add in fixed effects | Other developments . Empirical application | Monte Carlo simulation to test veracity and robustness of code | from propscore import PropensityScore import random import pandas as pd import sklearn import sklearn.model_selection import numpy as np import statistics as stats import statsmodels.api as sm from scipy.stats import norm import warnings warnings.filterwarnings(&quot;ignore&quot;, category=FutureWarning) . from causalinference import CausalModel from sklearn.linear_model import LinearRegression from sklearn.preprocessing import OneHotEncoder from sklearn.ensemble import RandomForestRegressor from sklearn import svm from sklearn import datasets, ensemble from sklearn.neural_network import MLPRegressor from sklearn.linear_model import ElasticNet . Data . df = pd.read_csv(&quot;~/OneDrive - London School of Economics/LSE/Year 3/EC331/November/simdata1.csv&quot;) # In this simulated dataset, all controls are uniformly random around (-1,1). Treatment (binary) is randomly assigned # and has a treatment effect of 2.0 + some gaussian noise. controls = [&#39;X1&#39;,&#39;X2&#39;,&#39;X3&#39;,&#39;X4&#39;,&#39;X5&#39;] treatment = &#39;treatment&#39; . ps = PropensityScore(treatment, controls, df); df = df.join(ps.propscore) df.head() . Logit Regression Results ============================================================================== Dep. Variable: treatment No. Observations: 2000 Model: Logit Df Residuals: 1996 Method: MLE Df Model: 3 Date: Mon, 04 Jan 2021 Pseudo R-squ.: 0.003082 Time: 14:12:22 Log-Likelihood: -1382.0 converged: True LL-Null: -1386.3 Covariance Type: nonrobust LLR p-value: 0.03598 ============================================================================== coef std err z P&gt;|z| [0.025 0.975] X4 0.0656 0.044 1.481 0.138 -0.021 0.152 X1XX2 -0.0862 0.046 -1.889 0.059 -0.176 0.003 X4_sq 0.0576 0.032 1.802 0.072 -0.005 0.120 _cons -0.0592 0.056 -1.066 0.286 -0.168 0.050 ============================================================================== The following vars were infeasible: Stratification produced 4 strata . X1 X2 X3 X4 X5 treatment outcome propscore . 0 | 0.222325 | 1.096420 | 0.791501 | -0.047723 | -0.638967 | 1 | 1.896553 | 0.479204 | . 1 | -0.333924 | -2.472571 | -0.635518 | 0.640664 | 0.408980 | 0 | -0.005217 | 0.483821 | . 2 | -0.635914 | -0.714505 | 0.170289 | -0.591279 | -0.211143 | 0 | 0.097441 | 0.470775 | . 3 | 0.618626 | -1.055120 | 0.486168 | -0.441626 | -2.865479 | 0 | 0.015330 | 0.494820 | . 4 | 0.807180 | 1.475266 | -0.033577 | 0.354557 | 0.868843 | 1 | 1.893319 | 0.467209 | . Initialisation . iterations = 100 k = 5 # number of groups for heterogeneity analysis alpha = 0.05 # significance level . Running everything . ML_models = [&quot;random_forest&quot;, &quot;SVM&quot;, &quot;gradient_boost&quot;, &quot;neural_net&quot;, &quot;ElasticNet&quot;] for x in ML_models: summary = Generic_ML_single(df, controls, 10, x, alpha , 5) print (str(x) + &quot;: lamda1: &quot; + str(summary[-2])+ &quot; lambda2: &quot; + str(summary[-1])) . random_forest: lamda1: -5.765410377113008e-05 lambda2: 0.7962556067959392 SVM: lamda1: 6.37687223615527e-05 lambda2: 0.7990199258698686 gradient_boost: lamda1: -5.810927679997994e-06 lambda2: 0.7972126884049128 neural_net: lamda1: -0.00013872698846416478 lambda2: 0.7969536495255202 ElasticNet: lamda1: 0.00038697229802813827 lambda2: 0.7977092031809006 . This allows us to quickly compare between different ML estimators. In particular, we want to minimise lambda1 and lambda2. . summary = Generic_ML_single(df, controls, iterations, &quot;random_forest&quot;, alpha , 5) . BLP = summary[0]; BLP . ATE HET . coeff | 2.002647 | 0.019719 | . se | 0.007427 | 0.074006 | . pvalue | 0.000000 | 0.987714 | . lower bound | 1.987861 | -0.120732 | . upper bound | 2.017433 | 0.162308 | . GATES = summary[1]; GATES . G1 G5 G1 - G5 . coeff | 1.995021 | 2.010402 | -0.012300 | . se | 0.017296 | 0.017322 | 0.024500 | . pvalue | 0.000000 | 0.000000 | 1.000000 | . lower bound | 1.960978 | 1.976616 | -0.061299 | . upper bound | 2.029177 | 2.044189 | 0.036699 | . CLAN = summary[2]; CLAN . coeff se pvalue lower bound upper bound . Most affected X1 | -0.046367 | 0.079034 | 0.393447 | -0.202160 | 0.109426 | . Least affected X1 | -0.025723 | 0.079034 | 0.456220 | -0.181875 | 0.130428 | . Most - least affected X1 | 0.033200 | 0.250000 | 0.693000 | -0.460711 | 0.527111 | . Most affected X2 | 0.026326 | 0.083156 | 0.602332 | -0.136445 | 0.189097 | . Least affected X2 | -0.007814 | 0.083156 | 0.345540 | -0.170585 | 0.154957 | . Most - least affected X2 | 0.034150 | 0.131500 | 0.620500 | -0.222605 | 0.290905 | . Most affected X3 | 0.051797 | 0.081971 | 0.473087 | -0.112999 | 0.216593 | . Least affected X3 | -0.010720 | 0.081971 | 0.637616 | -0.172631 | 0.151190 | . Most - least affected X3 | 0.049050 | 0.130000 | 0.542000 | -0.207705 | 0.305805 | . Most affected X4 | 0.015797 | 0.083432 | 0.578876 | -0.147815 | 0.179409 | . Least affected X4 | -0.031468 | 0.083432 | 0.494258 | -0.197107 | 0.134171 | . Most - least affected X4 | 0.039350 | 0.132000 | 0.532000 | -0.213485 | 0.292185 | . Most affected X5 | -0.008161 | 0.080664 | 0.324056 | -0.166095 | 0.149773 | . Least affected X5 | -0.012703 | 0.080664 | 0.338504 | -0.171438 | 0.146032 | . Most - least affected X5 | 0.010450 | 0.127500 | 0.414000 | -0.240425 | 0.261325 | . HELPER FUNCTIONS . BLP . def BLP(df, alpha): &#39;&#39;&#39; Returns summary results, whose parameters can be used to obtain BLP of CATE. Contains: Estimator Coefficients of Term 2 and 3 Standard Error p values Confidence Interval (lower and upper bounds) Returns lambda1 - value to help choose the best ML method &#39;&#39;&#39; term2 = df[&#39;treatment&#39;] - df[&#39;propscore&#39;] S = df[&#39;S&#39;] term3 = term2 * (S - np.mean(S)) combined = df.copy() combined.loc[:,&#39;term2&#39;] = term2 combined.loc[:,&#39;term3&#39;] = term3 combined.loc[:,&#39;ones&#39;] = 1 X_reg = combined[[&#39;B&#39;, &#39;S&#39;, &#39;ones&#39;, &#39;term2&#39;, &#39;term3&#39;]] y = combined[[&#39;outcome&#39;]] regBLP = sm.OLS(y, X_reg) res_BLP = regBLP.fit() res_BLP = results_summary_to_dataframe(res_BLP, alpha) lambda1 = res_BLP.iloc[-1,0] * stats.variance(S) return res_BLP, lambda1 def results_summary_to_dataframe(results, alpha): &#39;&#39;&#39;take the result of an statsmodel results table and transforms it into a dataframe&#39;&#39;&#39; pvals = results.pvalues coeff = results.params std_err = results.bse crit_val = norm.ppf(1-alpha/2) lb = coeff - std_err * crit_val ub = coeff + std_err * crit_val results_df = pd.DataFrame({&quot;pvals&quot;:pvals, &quot;coeff&quot;:coeff, &quot;lb&quot;:lb, &quot;ub&quot;:ub, &quot;std_err&quot;:std_err, }) #Reordering... results_df = results_df[[&quot;coeff&quot;,&quot;std_err&quot;,&quot;pvals&quot;,&quot;lb&quot;,&quot;ub&quot;]] return results_df def BLP_to_storage(res_BLP): &#39;&#39;&#39; Takes the output of BLP and store them as lists, whereby the output refers to: res_BLP - summary table containing parameters to construct BLP, along with their p-values, standard errors and lower and upper bounds Returns 2 lists data_HET and data_ATE whose array-equivalent is of dimension (1 variable, 5 attributes) &#39;&#39;&#39; # HET parameter HET = res_BLP.iloc[-1,0] HET_se = res_BLP.iloc[-1,1] HET_pvals = res_BLP.iloc[-1, 2] HET_lb = res_BLP.iloc[-1, 3] HET_ub = res_BLP.iloc[-1, 4] # ATE ATE = res_BLP.iloc[-2,0] ATE_se = res_BLP.iloc[-2,1] ATE_pvals = res_BLP.iloc[-2,2] ATE_lb = res_BLP.iloc[-2,3] ATE_ub = res_BLP.iloc[-2,4] # Storage data_HET = [HET, HET_se, HET_pvals, HET_lb, HET_ub] data_ATE = [ATE, ATE_se, ATE_pvals, ATE_lb, ATE_ub] return data_HET, data_ATE . GATES . def GATES(df, k , alpha): &#39;&#39;&#39; Returns summary statistics, whose results can give us the average treatment effect for most and least affected group. Contains: Estimator Coefficients Standard Error p values Confidence Interval (lower and upper bounds) Returns lambda2 - value to help choose the best ML method Parameters - df -- (main) dataframe which must contain the following items: propensity score B - proxy predictor for BCA S - proxy predictor for CATE treatment k -- number of groups &#39;&#39;&#39; combined = df.copy() term2 = df[&#39;treatment&#39;] - df[&#39;propscore&#39;] combined.loc[:,&#39;term2&#39;] = term2 combined.loc[:,&#39;ones&#39;] = 1 groups = groups_multiply(df, group_create(k, df), k) combined = pd.concat([combined,groups], axis = 1) controls = [&quot;B&quot;, &quot;S&quot;, &quot;ones&quot;] + [&quot;G&quot; + str(i) for i in range(1,k+1)] X_GATES = combined[controls] # modify for auto selection of columns y = combined[[&#39;outcome&#39;]] regGATES = sm.OLS(y, X_GATES) res_GATES = regGATES.fit() # Hypothesis testing hypothesis = &quot;(G1 = &quot; + &quot;G&quot; + str(k) + &quot;)&quot; # G1 = G{k} t_test_html = res_GATES.t_test(hypothesis).summary().as_html() t_test = pd.read_html(t_test_html, header=0, index_col=0)[0] res_GATES = results_summary_to_dataframe(res_GATES, alpha) lambda2 = res_GATES.iloc[3:, 0].mean()**2 / k return res_GATES, t_test, lambda2 def group_create(k, df): &#39;&#39;&#39; Returns quantiles of the variable &#39;S&#39;, encoded into dummy variables &#39;&#39;&#39; breaks = df[&#39;S&#39;].quantile(np.linspace(0,1,(k+1))) breaks.iloc[0,] = breaks.iloc[0,] - 0.001 breaks.iloc[k,] = breaks.iloc[k,] - 0.001 combined = df.copy() combined[&#39;Groups&#39;] = pd.cut(x= df[&#39;S&#39;], bins = breaks) # this will fail if there are too many groups groups = pd.get_dummies(combined[&#39;Groups&#39;]) return groups def groups_multiply(df, groups, k): &#39;&#39;&#39; Multiply groups dataframe with term 2 and rename columns &#39;&#39;&#39; combined = df.copy() term2 = df[&#39;treatment&#39;] - df[&#39;propscore&#39;] combined.loc[:,&#39;term2&#39;] = term2 groups = np.multiply(groups, combined[&#39;term2&#39;].values.reshape(len(df.index),1)) groups.columns = [&quot;G&quot; + str(i) for i in range(1,k+1)] return groups def GATES_to_storage(res_GATES, t_test_GATES, alpha): &#39;&#39;&#39; Takes the output of GATES and store them as lists, whereby the output refers to: res_GATES - summary table containing parameters to construct GATES, along with their p-values and standard errors t_test_GATEs - t test table to determine if G1 = Gk Returns a list whose array-equivalent is dimension of (# of variables, # of attributes ) &#39;&#39;&#39; # Most affected group gamma1 = res_GATES.iloc[3,0] gamma1_se = res_GATES.iloc[3,1] gamma1_pvals = res_GATES.iloc[3,2] gamma1_lb = res_GATES.iloc[3,3] gamma1_ub = res_GATES.iloc[3,4] # Least affected group gammak = res_GATES.iloc[-1,0] gammak_se = res_GATES.iloc[-1,1] gammak_pvals = res_GATES.iloc[-1,2] gammak_lb = res_GATES.iloc[-1,3] gammak_ub = res_GATES.iloc[-1,4] # Difference between most and least affected group crit_val = norm.ppf(1-alpha/2) gamma_diff = t_test_GATES.iloc[0,0] gamma_diff_se = t_test_GATES.iloc[0,1] gamma_diff_pvals = t_test_GATES.iloc[0,3] gamma_diff_lb = gamma_diff - crit_val * gamma_diff_se gamma_diff_ub = gamma_diff + crit_val * gamma_diff_se data_gamma1 = [gamma1, gamma1_se, gamma1_pvals, gamma1_lb, gamma1_ub] data_gammak = [gammak, gammak_se, gammak_pvals, gammak_lb, gammak_ub] data_gamma_diff = [gamma_diff, gamma_diff_se, gamma_diff_pvals, gamma_diff_lb, gamma_diff_ub] data_gamma = [data_gamma1, data_gammak, data_gamma_diff] return data_gamma . CLAN . def CLAN(df, controls, k = 5, alpha): data_CLAN_loop = [] for x in controls: res_CLAN, t_test = CLAN_single(df, x, k) data_CLAN = CLAN_to_storage(res_CLAN, t_test, alpha) data_CLAN_loop.append(data_CLAN) return data_CLAN_loop def CLAN_single(df, control, k = 5): &#39;&#39;&#39; Returns the average characteristic for one control between the most and least affected groups &#39;&#39;&#39; threshold = 1/k high_effect = df[&#39;S&#39;].quantile(1 - threshold) low_effect = df[&#39;S&#39;].quantile(threshold) combined = df.copy() combined.loc[:,&#39;high&#39;] = (combined.loc[:,&quot;S&quot;] &gt; high_effect).astype(int) # dummy variables for high combined.loc[:,&#39;low&#39;] = (combined.loc[:,&quot;S&quot;] &gt; low_effect).astype(int) # dummy variables for low combined.loc[:,&#39;minusones&#39;] = -1 X_control = combined[[&#39;high&#39;, &#39;low&#39;, &#39;minusones&#39;]] # I have no idea why I included minusones y_control = combined[[control]] reg_CLAN = sm.OLS(y_control, X_control) res_CLAN = reg_CLAN.fit() hypothesis = &quot;(high = low)&quot; t_test_html = res_CLAN.t_test(hypothesis).summary().as_html() t_test = pd.read_html(t_test_html, header=0, index_col=0)[0] res_CLAN = results_summary_to_dataframe(res_CLAN, alpha) return res_CLAN, t_test def CLAN_to_storage(res_CLAN, t_test, alpha): &#39;&#39;&#39; Takes the summary results of CLAN and its t test and store them as lists &#39;&#39;&#39; h_coeff = res_CLAN.iloc[0,0] h_se = res_CLAN.iloc[0,1] h_pvals = res_CLAN.iloc[0,2] h_lb = res_CLAN.iloc[0,3] h_ub = res_CLAN.iloc[0,4] data_h = [h_coeff, h_se, h_pvals, h_lb, h_ub] l_coeff = res_CLAN.iloc[1,0] l_se = res_CLAN.iloc[1,1] l_pvals = res_CLAN.iloc[1,2] l_lb = res_CLAN.iloc[1,3] l_ub = res_CLAN.iloc[1,4] data_l = [l_coeff, l_se, l_pvals, l_lb, l_ub] crit_val = norm.ppf(1-alpha/2) diff_coeff = t_test.iloc[0,0] diff_se = t_test.iloc[0,1] diff_pvals = t_test.iloc[0,3] diff_lb = diff_coeff - crit_val * diff_se diff_ub = diff_coeff + crit_val * diff_se data_diff = [diff_coeff, diff_se, diff_pvals, diff_lb, diff_ub] data_CLAN = data_h, data_l, data_diff return data_CLAN . Converting data into dataframes . def data_BLP_to_df(data_HET_loop, data_ATE_loop): &#39;&#39;&#39; Takes the data of BLP stored as a list, find its median over different iterations and adjusts p values. Returns it as a dataframe &#39;&#39;&#39; data_HET_array = np.array(data_HET_loop) data_HET_final = np.median(data_HET_array, axis = 0) data_HET_final[2] = np.minimum(1, data_HET_final[2] *2) data_ATE_array = np.array(data_ATE_loop) data_ATE_final = np.median(data_ATE_array, axis = 0) data_ATE_final[2] = np.minimum(1, data_ATE_final[2] * 2) df_ATE = pd.DataFrame(data_ATE_final, index = [&#39;coeff&#39;, &#39;se&#39;, &#39;pvalue&#39;, &#39;lower bound&#39;, &#39;upper bound&#39;], columns = [&#39;ATE&#39;]) df_HET = pd.DataFrame(data_HET_final, index = [&#39;coeff&#39;, &#39;se&#39;, &#39;pvalue&#39;, &#39;lower bound&#39;, &#39;upper bound&#39;], columns = [&#39;HET&#39;]) frames = [df_ATE, df_HET] df_BLP = pd.concat(frames, axis = 1) return df_BLP def data_GATES_to_df(data_GATES_loop, groups): &#39;&#39;&#39; Takes the data of GATES stored as a list, find its median over different iterations and adjusts p values. Returns it as a dataframe &#39;&#39;&#39; # GATES data_GATES_array = np.array(data_GATES_loop) data_GATES_final = np.median(data_GATES_array, axis = 0) data_GATES_final[:, 2] = np.minimum(1, data_GATES_final[:, 2]* 2) df_GATES = pd.DataFrame(data_GATES_final, columns = [&#39;coeff&#39;, &#39;se&#39;, &#39;pvalue&#39;, &#39;lower bound&#39;, &#39;upper bound&#39;], index = [&#39;G1&#39;, &quot;G&quot; + str(groups), &quot;G1 - G&quot; + str(groups)]) return df_GATES.transpose() def data_CLAN_to_df(data_CLAN_loop, controls = controls): &#39;&#39;&#39; Takes the data of GATES stored as a list, find its median over different iterations and adjusts p values. Returns it as a dataframe &#39;&#39;&#39; # CLAN data_CLAN_array = np.array(data_CLAN_loop) data_CLAN_final = np.median(data_CLAN_array, axis = 0) # This code is technically wrong as we take the upper medians for the lower bounds data_CLAN_final[0,2,:] = np.minimum(1, data_CLAN_final[0,2,:] * 2) list = [] for x in controls: list1 = [&#39;Most affected &#39; + str(x), &#39;Least affected &#39; + str(x), &#39;Most - least affected &#39; + str(x) ] list.append(list1) flattened_list = [y for x in list for y in x] data_CLAN_new = data_CLAN_final.reshape(-1,5) df_CLAN = pd.DataFrame(data_CLAN_new, columns = [&#39;coeff&#39;, &#39;se&#39;, &#39;pvalue&#39;, &#39;lower bound&#39;, &#39;upper bound&#39;], index = flattened_list) return df_CLAN . Putting everything together . def Generic_ML_single(df, controls, iterations = 10, model = &quot;random_forest&quot;, alpha = 0.05, k = 5): &#39;&#39;&#39; Runs the whole generic ML algorithm for a ML model and returns a list of datasets for all parameters. &#39;&#39;&#39; data_HET_loop = [] data_ATE_loop = [] lambda1_loop = [] data_GATES_loop = [] lambda2_loop = [] data_CLAN_loop = [] for x in range(iterations): main, aux = sklearn.model_selection.train_test_split(df, train_size = 0.5, random_state = x) main2 = ML_estimator(main, aux, model) # BLP res_BLP, lambda1 = BLP(main2) data_HET, data_ATE = BLP_to_storage(res_BLP) data_HET_loop.append(data_HET) data_ATE_loop.append(data_ATE) lambda1_loop.append(lambda1) #GATES res_GATES, t_test_GATES, lambda2 = GATES(main2, k, alpha) data_GATES = GATES_to_storage(res_GATES, t_test_GATES, alpha) data_GATES_loop.append(data_GATES) lambda2_loop.append(lambda2) # CLAN controls = controls data_CLAN = CLAN(main2, controls) data_CLAN_loop.append(data_CLAN) # BLP data_HET_array = np.array(data_HET_loop) data_HET_final = np.median(data_HET_array, axis = 0) data_HET_final[2] = np.minimum(1, data_HET_final[2] *2) data_ATE_array = np.array(data_ATE_loop) data_ATE_final = np.median(data_ATE_array, axis = 0) data_ATE_final[2] = np.minimum(1, data_ATE_final[2] * 2) df_BLP = data_BLP_to_df(data_HET_loop, data_ATE_loop) df_GATES = data_GATES_to_df(data_GATES_loop, k) df_CLAN = data_CLAN_to_df(data_CLAN_loop, controls = controls) lambda1 = np.mean(lambda1_loop) lamda2 = np.mean(lambda2_loop) summary = [df_BLP, df_GATES, df_CLAN, lambda1, lambda2] return summary . ML estimators . def ML_estimator(main, aux, model): &#39;&#39;&#39; Returns the main dataset combined with B and S, which are proxy predictors for BCA and CATE respectively Parameters - main: main dataset which must contain treatment and outcome aux: auxilliary dataset which must contain treatment and outcome model - in string format models = [&quot;random_forest&quot;, &quot;SVM&quot;, &quot;gradient_boost&quot;, &quot;neural_net&quot;, &quot;ElasticNet&quot;] # need to set the seed of the ML_estimators &#39;&#39;&#39; # Initialization aux0 = aux[aux[&#39;treatment&#39;] == 0] aux1 = aux[aux[&#39;treatment&#39;] == 1] X_aux0 = aux0[[&#39;treatment&#39;, &#39;X1&#39;, &#39;X2&#39;, &#39;X3&#39;, &#39;X4&#39;, &#39;X5&#39;]] y_aux0 =aux0[&#39;outcome&#39;] X_aux1 = aux1[[&#39;treatment&#39;, &#39;X1&#39;, &#39;X2&#39;, &#39;X3&#39;, &#39;X4&#39;, &#39;X5&#39;]] y_aux1 =aux1[&#39;outcome&#39;] X_main = main[[&#39;treatment&#39;, &#39;X1&#39;, &#39;X2&#39;, &#39;X3&#39;, &#39;X4&#39;, &#39;X5&#39;]] y_main = main[&#39;outcome&#39;] # Model if model == &quot;random_forest&quot;: combined = random_forest(main, X_aux0, y_aux0, X_main, X_aux1, y_aux1) elif model == &quot;SVM&quot;: combined = SVM(main, X_aux0, y_aux0, X_main, X_aux1, y_aux1) elif model == &quot;gradient_boost&quot;: combined = gradient_boost(main, X_aux0, y_aux0, X_main, X_aux1, y_aux1) elif model == &quot;neural_net&quot;: combined = neural_net(main, X_aux0, y_aux0, X_main, X_aux1, y_aux1) elif model == &quot;ElasticNet&quot;: combined = ElasticNet(main, X_aux0, y_aux0, X_main, X_aux1, y_aux1) # Add variance if stats.variance(combined[&#39;S&#39;]) == 0 : combined[&#39;S&#39;] = combined[&#39;S&#39;] + np.random.normal(0,0.1, len(combined[&#39;S&#39;])) if stats.variance(combined[&#39;B&#39;]) == 0 : combined[&#39;B&#39;] = combined[&#39;B&#39;] + np.random.normal(0,0.1, len(combined[&#39;B&#39;])) return combined def random_forest(main, X_aux0, y_aux0, X_main, X_aux1, y_aux1): # Model clf = RandomForestRegressor(max_depth=2, random_state=0) clf.fit(X_aux0, y_aux0) B = clf.predict(X_main) clf.fit(X_aux1, y_aux1) clf.predict(X_main) S = clf.predict(X_main) - B combined = main.copy() combined[&#39;B&#39;] = B combined[&#39;S&#39;] = S return combined def SVM(main, X_aux0, y_aux0, X_main, X_aux1, y_aux1): # Model clf = svm.SVR() clf.fit(X_aux0, y_aux0) B = clf.predict(X_main) clf.fit(X_aux1, y_aux1) clf.predict(X_main) S = clf.predict(X_main) - B combined = main.copy() combined[&#39;B&#39;] = B combined[&#39;S&#39;] = S return combined def gradient_boost(main, X_aux0, y_aux0, X_main, X_aux1, y_aux1): params = {&#39;n_estimators&#39;: 500, &#39;max_depth&#39;: 4, &#39;min_samples_split&#39;: 5, &#39;learning_rate&#39;: 0.01, &#39;loss&#39;: &#39;ls&#39;} # Model clf = ensemble.GradientBoostingRegressor(**params) clf.fit(X_aux0, y_aux0) B = clf.predict(X_main) clf.fit(X_aux1, y_aux1) clf.predict(X_main) S = clf.predict(X_main) - B combined = main.copy() combined[&#39;B&#39;] = B combined[&#39;S&#39;] = S return combined def neural_net(main, X_aux0, y_aux0, X_main, X_aux1, y_aux1): # Model clf = MLPRegressor(solver=&#39;lbfgs&#39;, alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1) clf.fit(X_aux0, y_aux0) B = clf.predict(X_main) clf.fit(X_aux1, y_aux1) clf.predict(X_main) S = clf.predict(X_main) - B combined = main.copy() combined[&#39;B&#39;] = B combined[&#39;S&#39;] = S return combined def ElasticNet(main, X_aux0, y_aux0, X_main, X_aux1, y_aux1): # Model clf = sklearn.linear_model.ElasticNet() clf.fit(X_aux0, y_aux0) B = clf.predict(X_main) clf.fit(X_aux1, y_aux1) clf.predict(X_main) S = clf.predict(X_main) - B combined = main.copy() combined[&#39;B&#39;] = B combined[&#39;S&#39;] = S return combined .",
            "url": "https://munfai15.github.io/genericML/fastpages/jupyter/2020/01/04/Generic-ML.html",
            "relUrl": "/fastpages/jupyter/2020/01/04/Generic-ML.html",
            "date": " • Jan 4, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am a final year student studying Econometrics and Mathematical Economics at the London School of Economics. . I am working on my dissertation to infer heterogeneous causality using Machine Learning, and would wish to extend my learning in data science / analytics / machine learning through future post-graduate studies. .",
          "url": "https://munfai15.github.io/genericML/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://munfai15.github.io/genericML/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}