{
  
    
        "post0": {
            "title": "Generic Machine Learning",
            "content": "This notebook provides code to Generic Machine Learning Inference on Heterogenous Treatment Effects in Randomized Experiments by Victor Chernozhukov, Mert Demirer, Esther Duflo, and Iván Fernández-Val. . https://arxiv.org/abs/1712.04802 . References . https://github.com/arnaudfrn/MLheterogeneity/blob/dev/src/vb_heterogeneity_FE.R . Many thanks to Arnaud Fournier, who provided the R code for this. . Author of notebook : Mun Fai Chan . Future developments for code . Hyperparemeter tuning on ML estimators | Converting pandas dataframes to LaTex tables. | Aesthetic updates - includes adding astericks for significance | Add in fixed effects | Other developments . Empirical application | Monte Carlo simulation to test veracity and robustness of code | from propscore import PropensityScore import random import pandas as pd import sklearn import sklearn.model_selection import numpy as np import statistics as stats import statsmodels.api as sm from scipy.stats import norm import warnings warnings.filterwarnings(&quot;ignore&quot;, category=FutureWarning) . from causalinference import CausalModel from sklearn.linear_model import LinearRegression from sklearn.preprocessing import OneHotEncoder from sklearn.ensemble import RandomForestRegressor from sklearn import svm from sklearn import datasets, ensemble from sklearn.neural_network import MLPRegressor from sklearn.linear_model import ElasticNet . Data . df = pd.read_csv(&quot;~/OneDrive - London School of Economics/LSE/Year 3/EC331/November/simdata1.csv&quot;) # In this simulated dataset, all controls are uniformly random around (-1,1). Treatment (binary) is randomly assigned # and has a treatment effect of 2.0 + some gaussian noise. controls = [&#39;X1&#39;,&#39;X2&#39;,&#39;X3&#39;,&#39;X4&#39;,&#39;X5&#39;] treatment = &#39;treatment&#39; . ps = PropensityScore(treatment, controls, df); df = df.join(ps.propscore) df.head() . Logit Regression Results ============================================================================== Dep. Variable: treatment No. Observations: 2000 Model: Logit Df Residuals: 1996 Method: MLE Df Model: 3 Date: Mon, 04 Jan 2021 Pseudo R-squ.: 0.003082 Time: 14:12:22 Log-Likelihood: -1382.0 converged: True LL-Null: -1386.3 Covariance Type: nonrobust LLR p-value: 0.03598 ============================================================================== coef std err z P&gt;|z| [0.025 0.975] X4 0.0656 0.044 1.481 0.138 -0.021 0.152 X1XX2 -0.0862 0.046 -1.889 0.059 -0.176 0.003 X4_sq 0.0576 0.032 1.802 0.072 -0.005 0.120 _cons -0.0592 0.056 -1.066 0.286 -0.168 0.050 ============================================================================== The following vars were infeasible: Stratification produced 4 strata . X1 X2 X3 X4 X5 treatment outcome propscore . 0 | 0.222325 | 1.096420 | 0.791501 | -0.047723 | -0.638967 | 1 | 1.896553 | 0.479204 | . 1 | -0.333924 | -2.472571 | -0.635518 | 0.640664 | 0.408980 | 0 | -0.005217 | 0.483821 | . 2 | -0.635914 | -0.714505 | 0.170289 | -0.591279 | -0.211143 | 0 | 0.097441 | 0.470775 | . 3 | 0.618626 | -1.055120 | 0.486168 | -0.441626 | -2.865479 | 0 | 0.015330 | 0.494820 | . 4 | 0.807180 | 1.475266 | -0.033577 | 0.354557 | 0.868843 | 1 | 1.893319 | 0.467209 | . Initialisation . iterations = 100 k = 5 # number of groups for heterogeneity analysis alpha = 0.05 # significance level . Running everything . ML_models = [&quot;random_forest&quot;, &quot;SVM&quot;, &quot;gradient_boost&quot;, &quot;neural_net&quot;, &quot;ElasticNet&quot;] for x in ML_models: summary = Generic_ML_single(df, controls, 10, x, alpha , 5) print (str(x) + &quot;: lamda1: &quot; + str(summary[-2])+ &quot; lambda2: &quot; + str(summary[-1])) . random_forest: lamda1: -5.765410377113008e-05 lambda2: 0.7962556067959392 SVM: lamda1: 6.37687223615527e-05 lambda2: 0.7990199258698686 gradient_boost: lamda1: -5.810927679997994e-06 lambda2: 0.7972126884049128 neural_net: lamda1: -0.00013872698846416478 lambda2: 0.7969536495255202 ElasticNet: lamda1: 0.00038697229802813827 lambda2: 0.7977092031809006 . This allows us to quickly compare between different ML estimators. In particular, we want to minimise lambda1 and lambda2. . summary = Generic_ML_single(df, controls, iterations, &quot;random_forest&quot;, alpha , 5) . BLP = summary[0]; BLP . ATE HET . coeff | 2.002647 | 0.019719 | . se | 0.007427 | 0.074006 | . pvalue | 0.000000 | 0.987714 | . lower bound | 1.987861 | -0.120732 | . upper bound | 2.017433 | 0.162308 | . GATES = summary[1]; GATES . G1 G5 G1 - G5 . coeff | 1.995021 | 2.010402 | -0.012300 | . se | 0.017296 | 0.017322 | 0.024500 | . pvalue | 0.000000 | 0.000000 | 1.000000 | . lower bound | 1.960978 | 1.976616 | -0.061299 | . upper bound | 2.029177 | 2.044189 | 0.036699 | . CLAN = summary[2]; CLAN . coeff se pvalue lower bound upper bound . Most affected X1 | -0.046367 | 0.079034 | 0.393447 | -0.202160 | 0.109426 | . Least affected X1 | -0.025723 | 0.079034 | 0.456220 | -0.181875 | 0.130428 | . Most - least affected X1 | 0.033200 | 0.250000 | 0.693000 | -0.460711 | 0.527111 | . Most affected X2 | 0.026326 | 0.083156 | 0.602332 | -0.136445 | 0.189097 | . Least affected X2 | -0.007814 | 0.083156 | 0.345540 | -0.170585 | 0.154957 | . Most - least affected X2 | 0.034150 | 0.131500 | 0.620500 | -0.222605 | 0.290905 | . Most affected X3 | 0.051797 | 0.081971 | 0.473087 | -0.112999 | 0.216593 | . Least affected X3 | -0.010720 | 0.081971 | 0.637616 | -0.172631 | 0.151190 | . Most - least affected X3 | 0.049050 | 0.130000 | 0.542000 | -0.207705 | 0.305805 | . Most affected X4 | 0.015797 | 0.083432 | 0.578876 | -0.147815 | 0.179409 | . Least affected X4 | -0.031468 | 0.083432 | 0.494258 | -0.197107 | 0.134171 | . Most - least affected X4 | 0.039350 | 0.132000 | 0.532000 | -0.213485 | 0.292185 | . Most affected X5 | -0.008161 | 0.080664 | 0.324056 | -0.166095 | 0.149773 | . Least affected X5 | -0.012703 | 0.080664 | 0.338504 | -0.171438 | 0.146032 | . Most - least affected X5 | 0.010450 | 0.127500 | 0.414000 | -0.240425 | 0.261325 | . HELPER FUNCTIONS . BLP . def BLP(df, alpha): &#39;&#39;&#39; Returns summary results, whose parameters can be used to obtain BLP of CATE. Contains: Estimator Coefficients of Term 2 and 3 Standard Error p values Confidence Interval (lower and upper bounds) Returns lambda1 - value to help choose the best ML method &#39;&#39;&#39; term2 = df[&#39;treatment&#39;] - df[&#39;propscore&#39;] S = df[&#39;S&#39;] term3 = term2 * (S - np.mean(S)) combined = df.copy() combined.loc[:,&#39;term2&#39;] = term2 combined.loc[:,&#39;term3&#39;] = term3 combined.loc[:,&#39;ones&#39;] = 1 X_reg = combined[[&#39;B&#39;, &#39;S&#39;, &#39;ones&#39;, &#39;term2&#39;, &#39;term3&#39;]] y = combined[[&#39;outcome&#39;]] regBLP = sm.OLS(y, X_reg) res_BLP = regBLP.fit() res_BLP = results_summary_to_dataframe(res_BLP, alpha) lambda1 = res_BLP.iloc[-1,0] * stats.variance(S) return res_BLP, lambda1 def results_summary_to_dataframe(results, alpha): &#39;&#39;&#39;take the result of an statsmodel results table and transforms it into a dataframe&#39;&#39;&#39; pvals = results.pvalues coeff = results.params std_err = results.bse crit_val = norm.ppf(1-alpha/2) lb = coeff - std_err * crit_val ub = coeff + std_err * crit_val results_df = pd.DataFrame({&quot;pvals&quot;:pvals, &quot;coeff&quot;:coeff, &quot;lb&quot;:lb, &quot;ub&quot;:ub, &quot;std_err&quot;:std_err, }) #Reordering... results_df = results_df[[&quot;coeff&quot;,&quot;std_err&quot;,&quot;pvals&quot;,&quot;lb&quot;,&quot;ub&quot;]] return results_df def BLP_to_storage(res_BLP): &#39;&#39;&#39; Takes the output of BLP and store them as lists, whereby the output refers to: res_BLP - summary table containing parameters to construct BLP, along with their p-values, standard errors and lower and upper bounds Returns 2 lists data_HET and data_ATE whose array-equivalent is of dimension (1 variable, 5 attributes) &#39;&#39;&#39; # HET parameter HET = res_BLP.iloc[-1,0] HET_se = res_BLP.iloc[-1,1] HET_pvals = res_BLP.iloc[-1, 2] HET_lb = res_BLP.iloc[-1, 3] HET_ub = res_BLP.iloc[-1, 4] # ATE ATE = res_BLP.iloc[-2,0] ATE_se = res_BLP.iloc[-2,1] ATE_pvals = res_BLP.iloc[-2,2] ATE_lb = res_BLP.iloc[-2,3] ATE_ub = res_BLP.iloc[-2,4] # Storage data_HET = [HET, HET_se, HET_pvals, HET_lb, HET_ub] data_ATE = [ATE, ATE_se, ATE_pvals, ATE_lb, ATE_ub] return data_HET, data_ATE . GATES . def GATES(df, k , alpha): &#39;&#39;&#39; Returns summary statistics, whose results can give us the average treatment effect for most and least affected group. Contains: Estimator Coefficients Standard Error p values Confidence Interval (lower and upper bounds) Returns lambda2 - value to help choose the best ML method Parameters - df -- (main) dataframe which must contain the following items: propensity score B - proxy predictor for BCA S - proxy predictor for CATE treatment k -- number of groups &#39;&#39;&#39; combined = df.copy() term2 = df[&#39;treatment&#39;] - df[&#39;propscore&#39;] combined.loc[:,&#39;term2&#39;] = term2 combined.loc[:,&#39;ones&#39;] = 1 groups = groups_multiply(df, group_create(k, df), k) combined = pd.concat([combined,groups], axis = 1) controls = [&quot;B&quot;, &quot;S&quot;, &quot;ones&quot;] + [&quot;G&quot; + str(i) for i in range(1,k+1)] X_GATES = combined[controls] # modify for auto selection of columns y = combined[[&#39;outcome&#39;]] regGATES = sm.OLS(y, X_GATES) res_GATES = regGATES.fit() # Hypothesis testing hypothesis = &quot;(G1 = &quot; + &quot;G&quot; + str(k) + &quot;)&quot; # G1 = G{k} t_test_html = res_GATES.t_test(hypothesis).summary().as_html() t_test = pd.read_html(t_test_html, header=0, index_col=0)[0] res_GATES = results_summary_to_dataframe(res_GATES, alpha) lambda2 = res_GATES.iloc[3:, 0].mean()**2 / k return res_GATES, t_test, lambda2 def group_create(k, df): &#39;&#39;&#39; Returns quantiles of the variable &#39;S&#39;, encoded into dummy variables &#39;&#39;&#39; breaks = df[&#39;S&#39;].quantile(np.linspace(0,1,(k+1))) breaks.iloc[0,] = breaks.iloc[0,] - 0.001 breaks.iloc[k,] = breaks.iloc[k,] - 0.001 combined = df.copy() combined[&#39;Groups&#39;] = pd.cut(x= df[&#39;S&#39;], bins = breaks) # this will fail if there are too many groups groups = pd.get_dummies(combined[&#39;Groups&#39;]) return groups def groups_multiply(df, groups, k): &#39;&#39;&#39; Multiply groups dataframe with term 2 and rename columns &#39;&#39;&#39; combined = df.copy() term2 = df[&#39;treatment&#39;] - df[&#39;propscore&#39;] combined.loc[:,&#39;term2&#39;] = term2 groups = np.multiply(groups, combined[&#39;term2&#39;].values.reshape(len(df.index),1)) groups.columns = [&quot;G&quot; + str(i) for i in range(1,k+1)] return groups def GATES_to_storage(res_GATES, t_test_GATES, alpha): &#39;&#39;&#39; Takes the output of GATES and store them as lists, whereby the output refers to: res_GATES - summary table containing parameters to construct GATES, along with their p-values and standard errors t_test_GATEs - t test table to determine if G1 = Gk Returns a list whose array-equivalent is dimension of (# of variables, # of attributes ) &#39;&#39;&#39; # Most affected group gamma1 = res_GATES.iloc[3,0] gamma1_se = res_GATES.iloc[3,1] gamma1_pvals = res_GATES.iloc[3,2] gamma1_lb = res_GATES.iloc[3,3] gamma1_ub = res_GATES.iloc[3,4] # Least affected group gammak = res_GATES.iloc[-1,0] gammak_se = res_GATES.iloc[-1,1] gammak_pvals = res_GATES.iloc[-1,2] gammak_lb = res_GATES.iloc[-1,3] gammak_ub = res_GATES.iloc[-1,4] # Difference between most and least affected group crit_val = norm.ppf(1-alpha/2) gamma_diff = t_test_GATES.iloc[0,0] gamma_diff_se = t_test_GATES.iloc[0,1] gamma_diff_pvals = t_test_GATES.iloc[0,3] gamma_diff_lb = gamma_diff - crit_val * gamma_diff_se gamma_diff_ub = gamma_diff + crit_val * gamma_diff_se data_gamma1 = [gamma1, gamma1_se, gamma1_pvals, gamma1_lb, gamma1_ub] data_gammak = [gammak, gammak_se, gammak_pvals, gammak_lb, gammak_ub] data_gamma_diff = [gamma_diff, gamma_diff_se, gamma_diff_pvals, gamma_diff_lb, gamma_diff_ub] data_gamma = [data_gamma1, data_gammak, data_gamma_diff] return data_gamma . CLAN . def CLAN(df, controls, k = 5, alpha): data_CLAN_loop = [] for x in controls: res_CLAN, t_test = CLAN_single(df, x, k) data_CLAN = CLAN_to_storage(res_CLAN, t_test, alpha) data_CLAN_loop.append(data_CLAN) return data_CLAN_loop def CLAN_single(df, control, k = 5): &#39;&#39;&#39; Returns the average characteristic for one control between the most and least affected groups &#39;&#39;&#39; threshold = 1/k high_effect = df[&#39;S&#39;].quantile(1 - threshold) low_effect = df[&#39;S&#39;].quantile(threshold) combined = df.copy() combined.loc[:,&#39;high&#39;] = (combined.loc[:,&quot;S&quot;] &gt; high_effect).astype(int) # dummy variables for high combined.loc[:,&#39;low&#39;] = (combined.loc[:,&quot;S&quot;] &gt; low_effect).astype(int) # dummy variables for low combined.loc[:,&#39;minusones&#39;] = -1 X_control = combined[[&#39;high&#39;, &#39;low&#39;, &#39;minusones&#39;]] # I have no idea why I included minusones y_control = combined[[control]] reg_CLAN = sm.OLS(y_control, X_control) res_CLAN = reg_CLAN.fit() hypothesis = &quot;(high = low)&quot; t_test_html = res_CLAN.t_test(hypothesis).summary().as_html() t_test = pd.read_html(t_test_html, header=0, index_col=0)[0] res_CLAN = results_summary_to_dataframe(res_CLAN, alpha) return res_CLAN, t_test def CLAN_to_storage(res_CLAN, t_test, alpha): &#39;&#39;&#39; Takes the summary results of CLAN and its t test and store them as lists &#39;&#39;&#39; h_coeff = res_CLAN.iloc[0,0] h_se = res_CLAN.iloc[0,1] h_pvals = res_CLAN.iloc[0,2] h_lb = res_CLAN.iloc[0,3] h_ub = res_CLAN.iloc[0,4] data_h = [h_coeff, h_se, h_pvals, h_lb, h_ub] l_coeff = res_CLAN.iloc[1,0] l_se = res_CLAN.iloc[1,1] l_pvals = res_CLAN.iloc[1,2] l_lb = res_CLAN.iloc[1,3] l_ub = res_CLAN.iloc[1,4] data_l = [l_coeff, l_se, l_pvals, l_lb, l_ub] crit_val = norm.ppf(1-alpha/2) diff_coeff = t_test.iloc[0,0] diff_se = t_test.iloc[0,1] diff_pvals = t_test.iloc[0,3] diff_lb = diff_coeff - crit_val * diff_se diff_ub = diff_coeff + crit_val * diff_se data_diff = [diff_coeff, diff_se, diff_pvals, diff_lb, diff_ub] data_CLAN = data_h, data_l, data_diff return data_CLAN . Converting data into dataframes . def data_BLP_to_df(data_HET_loop, data_ATE_loop): &#39;&#39;&#39; Takes the data of BLP stored as a list, find its median over different iterations and adjusts p values. Returns it as a dataframe &#39;&#39;&#39; data_HET_array = np.array(data_HET_loop) data_HET_final = np.median(data_HET_array, axis = 0) data_HET_final[2] = np.minimum(1, data_HET_final[2] *2) data_ATE_array = np.array(data_ATE_loop) data_ATE_final = np.median(data_ATE_array, axis = 0) data_ATE_final[2] = np.minimum(1, data_ATE_final[2] * 2) df_ATE = pd.DataFrame(data_ATE_final, index = [&#39;coeff&#39;, &#39;se&#39;, &#39;pvalue&#39;, &#39;lower bound&#39;, &#39;upper bound&#39;], columns = [&#39;ATE&#39;]) df_HET = pd.DataFrame(data_HET_final, index = [&#39;coeff&#39;, &#39;se&#39;, &#39;pvalue&#39;, &#39;lower bound&#39;, &#39;upper bound&#39;], columns = [&#39;HET&#39;]) frames = [df_ATE, df_HET] df_BLP = pd.concat(frames, axis = 1) return df_BLP def data_GATES_to_df(data_GATES_loop, groups): &#39;&#39;&#39; Takes the data of GATES stored as a list, find its median over different iterations and adjusts p values. Returns it as a dataframe &#39;&#39;&#39; # GATES data_GATES_array = np.array(data_GATES_loop) data_GATES_final = np.median(data_GATES_array, axis = 0) data_GATES_final[:, 2] = np.minimum(1, data_GATES_final[:, 2]* 2) df_GATES = pd.DataFrame(data_GATES_final, columns = [&#39;coeff&#39;, &#39;se&#39;, &#39;pvalue&#39;, &#39;lower bound&#39;, &#39;upper bound&#39;], index = [&#39;G1&#39;, &quot;G&quot; + str(groups), &quot;G1 - G&quot; + str(groups)]) return df_GATES.transpose() def data_CLAN_to_df(data_CLAN_loop, controls = controls): &#39;&#39;&#39; Takes the data of GATES stored as a list, find its median over different iterations and adjusts p values. Returns it as a dataframe &#39;&#39;&#39; # CLAN data_CLAN_array = np.array(data_CLAN_loop) data_CLAN_final = np.median(data_CLAN_array, axis = 0) # This code is technically wrong as we take the upper medians for the lower bounds data_CLAN_final[0,2,:] = np.minimum(1, data_CLAN_final[0,2,:] * 2) list = [] for x in controls: list1 = [&#39;Most affected &#39; + str(x), &#39;Least affected &#39; + str(x), &#39;Most - least affected &#39; + str(x) ] list.append(list1) flattened_list = [y for x in list for y in x] data_CLAN_new = data_CLAN_final.reshape(-1,5) df_CLAN = pd.DataFrame(data_CLAN_new, columns = [&#39;coeff&#39;, &#39;se&#39;, &#39;pvalue&#39;, &#39;lower bound&#39;, &#39;upper bound&#39;], index = flattened_list) return df_CLAN . Putting everything together . def Generic_ML_single(df, controls, iterations = 10, model = &quot;random_forest&quot;, alpha = 0.05, k = 5): &#39;&#39;&#39; Runs the whole generic ML algorithm for a ML model and returns a list of datasets for all parameters. &#39;&#39;&#39; data_HET_loop = [] data_ATE_loop = [] lambda1_loop = [] data_GATES_loop = [] lambda2_loop = [] data_CLAN_loop = [] for x in range(iterations): main, aux = sklearn.model_selection.train_test_split(df, train_size = 0.5, random_state = x) main2 = ML_estimator(main, aux, model) # BLP res_BLP, lambda1 = BLP(main2) data_HET, data_ATE = BLP_to_storage(res_BLP) data_HET_loop.append(data_HET) data_ATE_loop.append(data_ATE) lambda1_loop.append(lambda1) #GATES res_GATES, t_test_GATES, lambda2 = GATES(main2, k, alpha) data_GATES = GATES_to_storage(res_GATES, t_test_GATES, alpha) data_GATES_loop.append(data_GATES) lambda2_loop.append(lambda2) # CLAN controls = controls data_CLAN = CLAN(main2, controls) data_CLAN_loop.append(data_CLAN) # BLP data_HET_array = np.array(data_HET_loop) data_HET_final = np.median(data_HET_array, axis = 0) data_HET_final[2] = np.minimum(1, data_HET_final[2] *2) data_ATE_array = np.array(data_ATE_loop) data_ATE_final = np.median(data_ATE_array, axis = 0) data_ATE_final[2] = np.minimum(1, data_ATE_final[2] * 2) df_BLP = data_BLP_to_df(data_HET_loop, data_ATE_loop) df_GATES = data_GATES_to_df(data_GATES_loop, k) df_CLAN = data_CLAN_to_df(data_CLAN_loop, controls = controls) lambda1 = np.mean(lambda1_loop) lamda2 = np.mean(lambda2_loop) summary = [df_BLP, df_GATES, df_CLAN, lambda1, lambda2] return summary . ML estimators . def ML_estimator(main, aux, model): &#39;&#39;&#39; Returns the main dataset combined with B and S, which are proxy predictors for BCA and CATE respectively Parameters - main: main dataset which must contain treatment and outcome aux: auxilliary dataset which must contain treatment and outcome model - in string format models = [&quot;random_forest&quot;, &quot;SVM&quot;, &quot;gradient_boost&quot;, &quot;neural_net&quot;, &quot;ElasticNet&quot;] # need to set the seed of the ML_estimators &#39;&#39;&#39; # Initialization aux0 = aux[aux[&#39;treatment&#39;] == 0] aux1 = aux[aux[&#39;treatment&#39;] == 1] X_aux0 = aux0[[&#39;treatment&#39;, &#39;X1&#39;, &#39;X2&#39;, &#39;X3&#39;, &#39;X4&#39;, &#39;X5&#39;]] y_aux0 =aux0[&#39;outcome&#39;] X_aux1 = aux1[[&#39;treatment&#39;, &#39;X1&#39;, &#39;X2&#39;, &#39;X3&#39;, &#39;X4&#39;, &#39;X5&#39;]] y_aux1 =aux1[&#39;outcome&#39;] X_main = main[[&#39;treatment&#39;, &#39;X1&#39;, &#39;X2&#39;, &#39;X3&#39;, &#39;X4&#39;, &#39;X5&#39;]] y_main = main[&#39;outcome&#39;] # Model if model == &quot;random_forest&quot;: combined = random_forest(main, X_aux0, y_aux0, X_main, X_aux1, y_aux1) elif model == &quot;SVM&quot;: combined = SVM(main, X_aux0, y_aux0, X_main, X_aux1, y_aux1) elif model == &quot;gradient_boost&quot;: combined = gradient_boost(main, X_aux0, y_aux0, X_main, X_aux1, y_aux1) elif model == &quot;neural_net&quot;: combined = neural_net(main, X_aux0, y_aux0, X_main, X_aux1, y_aux1) elif model == &quot;ElasticNet&quot;: combined = ElasticNet(main, X_aux0, y_aux0, X_main, X_aux1, y_aux1) # Add variance if stats.variance(combined[&#39;S&#39;]) == 0 : combined[&#39;S&#39;] = combined[&#39;S&#39;] + np.random.normal(0,0.1, len(combined[&#39;S&#39;])) if stats.variance(combined[&#39;B&#39;]) == 0 : combined[&#39;B&#39;] = combined[&#39;B&#39;] + np.random.normal(0,0.1, len(combined[&#39;B&#39;])) return combined def random_forest(main, X_aux0, y_aux0, X_main, X_aux1, y_aux1): # Model clf = RandomForestRegressor(max_depth=2, random_state=0) clf.fit(X_aux0, y_aux0) B = clf.predict(X_main) clf.fit(X_aux1, y_aux1) clf.predict(X_main) S = clf.predict(X_main) - B combined = main.copy() combined[&#39;B&#39;] = B combined[&#39;S&#39;] = S return combined def SVM(main, X_aux0, y_aux0, X_main, X_aux1, y_aux1): # Model clf = svm.SVR() clf.fit(X_aux0, y_aux0) B = clf.predict(X_main) clf.fit(X_aux1, y_aux1) clf.predict(X_main) S = clf.predict(X_main) - B combined = main.copy() combined[&#39;B&#39;] = B combined[&#39;S&#39;] = S return combined def gradient_boost(main, X_aux0, y_aux0, X_main, X_aux1, y_aux1): params = {&#39;n_estimators&#39;: 500, &#39;max_depth&#39;: 4, &#39;min_samples_split&#39;: 5, &#39;learning_rate&#39;: 0.01, &#39;loss&#39;: &#39;ls&#39;} # Model clf = ensemble.GradientBoostingRegressor(**params) clf.fit(X_aux0, y_aux0) B = clf.predict(X_main) clf.fit(X_aux1, y_aux1) clf.predict(X_main) S = clf.predict(X_main) - B combined = main.copy() combined[&#39;B&#39;] = B combined[&#39;S&#39;] = S return combined def neural_net(main, X_aux0, y_aux0, X_main, X_aux1, y_aux1): # Model clf = MLPRegressor(solver=&#39;lbfgs&#39;, alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1) clf.fit(X_aux0, y_aux0) B = clf.predict(X_main) clf.fit(X_aux1, y_aux1) clf.predict(X_main) S = clf.predict(X_main) - B combined = main.copy() combined[&#39;B&#39;] = B combined[&#39;S&#39;] = S return combined def ElasticNet(main, X_aux0, y_aux0, X_main, X_aux1, y_aux1): # Model clf = sklearn.linear_model.ElasticNet() clf.fit(X_aux0, y_aux0) B = clf.predict(X_main) clf.fit(X_aux1, y_aux1) clf.predict(X_main) S = clf.predict(X_main) - B combined = main.copy() combined[&#39;B&#39;] = B combined[&#39;S&#39;] = S return combined .",
            "url": "https://munfai15.github.io/genericML/fastpages/jupyter/2020/01/04/Generic-ML.html",
            "relUrl": "/fastpages/jupyter/2020/01/04/Generic-ML.html",
            "date": " • Jan 4, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am a final year student studying Econometrics and Mathematical Economics at the London School of Economics. . I am working on my dissertation to infer heterogeneous causality using Machine Learning, and would wish to extend my learning in data science / analytics / machine learning through future post-graduate studies. .",
          "url": "https://munfai15.github.io/genericML/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://munfai15.github.io/genericML/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}