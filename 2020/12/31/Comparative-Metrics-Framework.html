<p>toc: true
layout: post
description: Comparative Metrics Framework to evaluate the performance of synthetic data
categories: [markdown]
title: Comparative Metrics Framework</p>

<p>This abridged document[1] presents a comprehensive <em>comparative metrics framework</em> to evaluate the performance of synthetic data based on general utility, specific utility and disclosure risk. This is one of the few publicly available benchmarks to evaluate the quality of synthetic data. I will also provide some recommendations on best practices for generating synthetic data.</p>

<h1 id="introduction">Introduction</h1>

<p>I introduce a <em>comparative metrics framework</em> heavily inspired by Taub et. al (2019). Rather than reinventing the wheel, my focus here is to provide a practical guide to its implementation. In the event where certain code were not previously publicly available, I uploaded them into an R package <strong>cmf</strong> [2].</p>

<p>The functions used here are either from the <strong>cmf</strong> package or the synthpop package, which is a package to synthesise data. However, these functions can be generalised to any synthetic data generated with any method.</p>

<h1 id="general-utility">General Utility</h1>

<p>General utility is a measure of the similarity in distributions between synthetic and original data. A visual examination of histograms or frequency tables will be useful as a preliminary check, but becomes too infeasible when comparing between a large number of synthetic datasets. Instead, a single summary statistic score is necessary for easy comparison of performance.</p>

<h2 id="ratio-of-countsroc">Ratio of Counts(ROC)</h2>

<p>This evaluates the overlap in frequency counts between the original and synthetic dataset. I coded the mathematical equations introduced by Taub et el. (2020) in the <strong>cmf</strong> package.</p>

<p>[\frac{
min(y_{o}, y_{s})
}
{max(y_{o}, y_{s})
}]</p>

<p>(y_{o}) denotes the frequency count (by proportion) for a particular variable of the original dataset. (y_{s}) denotes the same for a synthetic dataset. Intuitively, this is analogous to the intersect of the 2 datasets divided by their union. The ROC score is bounded by 1 and 0. The lower the ROC score, the more distant the 2 datasets are in distribution. The function ROC_score evaluates the ROC score for all individual columns and averages them in the final output.</p>

<p>ROC_numeric evaluates the ROC score for numerical variables, because continuous numerical values in the original and synthetic data may not match exactly. For instance, 10,000 may be arbitrarily synthesised as 10,001 in the synthetic dataset but one may choose to treat them as the same value. Hence, ROC_numeric rounds numeric values off to a user-specified value.</p>

<h2 id="propensity-scores">Propensity Scores</h2>

<p>Intuitively, propensity score is the probability of a row of data being synthetic given its entries. One can select the column(s) to calculate the propensity score on. A higher ratio to degree of freedom score obtained from synthpop::utility.tab indicates a worse general utility. Ratios bigger than 3 or 4 suggests that a variable is problematic and may warrant further attention - visual examination of these columns is useful.</p>

<h1 id="specific-utility">Specific Utility</h1>

<p>Specific utility evaluates if we get similar estimates when running the same statistical analysis on both synthetic and original datasets.</p>

<h2 id="confidence-interval-overlap-cio">Confidence Interval Overlap (CIO)</h2>

<p>Here, I focus on logistic and linear regressions, though we can easily extend this to other machine learning inferences. I make use of the synthpop::lm.synds() and synthpop::glm.synds() functions to run regressions(linear and logistic respectively) on the original and synthetic dataset and evaluate how close confidence intervals are.</p>

<h1 id="disclosure-risk">Disclosure Risk</h1>

<h2 id="differential-correct-attribution-probabibility-dcap">Differential Correct Attribution Probabibility (DCAP)</h2>

<p>I coded functions for DCAP (Taub et al, 2018) in the R package <strong>cmf</strong>. DCAP works on the assumption that an intruder has certain <em>key information</em> about individuals. Such key information are highly observable or obtainable - for instance a person’s gender. Using these key information, the intruder attempts to extract <em>target information</em> that are more private - for instance a person’s income level. We define (d_{o}) as the original data and (K_{o}) and (T_{o}) as vectors for the key and target variables.</p>

<p>[d_{o} = {K_{o} , T_{o}}]</p>

<p>Similarly, (d_{s}) is the synthetic data.</p>

<p>[d_{s} = {K_{s} , T_{s}}]</p>

<p>The Correct Attribution Probability (CAP) for the record (j) is the probability of its target variables given its key variables. This is coded as the function CAP_original . We can think of the CAP score for the original dataset to be an approximate upper bound - it would not make much sense for any other dataset to reveal more information than the original dataset.</p>

<p>[CAP_{o,j} = Pr(T_{o,j} | K_{o,j}) = \frac{
\sum\limits_{i=1}^n 
[T_{o,i} = T_{o,j} ,  K_{o,i} = K_{o,j}] }
{\sum\limits_{i=1}^n 
[ K_{o,i} = K_{o,j}]}]</p>

<p>where the square brackets are Iverson brackets and (n) is the number of records.</p>

<p>The CAP for the record (j) based on a corresponding synthetic dataset (d_{s}) is the same probability but derived from (d_{s}). This has been coded as CAP_synthetic.</p>

<p>[CAP_{s,j} = Pr(T_{o,j} | K_{o,j})<em>{s}
= \frac{
\sum\limits</em>{i=1}^n 
[T_{s,i} = T_{o,j} ,  K_{s,i} = K_{o,j}] }
{\sum\limits_{i=1}^n 
[ K_{s,i} = K_{o,j}]}]</p>

<p>For any record in the original dataset for which there is no corresponding record in the synthetic dataset with the same key variables, the denominator in Equation (5) will be 0 and the CAP is therefore undefined. We can record non-matches as zero or treat them as undefined; the record is skipped over and does not count towards (n).</p>

<p>To calculate the overall CAP for a synthetic dataset, we divide its CAP score by its corresponding upper bound - the CAP score for the original dataset. This process is repeated for all combinations of key and target variables.</p>

<h1 id="provisional-findings-and-recommendations">Provisional Findings and Recommendations</h1>

<p>I then apply the framework to analyse 3 different datasets. However, due to their sensitivity, I cannot reference them publicly and can only provide provisional recommendations for best practices.</p>

<h2 id="rules-should-be-the-foremost-consideration-in-data-synthesis">Rules should be the foremost consideration in data synthesis.</h2>

<p>When relationships between variables are violated in the synthetic data, they lead to patently spurious results. For instance, if all individuals below (16) should be single, we have to check that this relationship holds in the synthetic datataset. Synthpop allows for the assertion of rules during the synthesis process which addresses this problem.</p>

<h2 id="visit-sequences-do-not-appear-to-have-a-significant-and-consistent-impact-on-performance">Visit sequences do not appear to have a significant and consistent impact on performance.</h2>

<p>A visit sequence is the order in which variables are synthesised. Visit sequences matter to the performance of a synthetic data only insofar as multiple visit sequences generate random variations which increase the likelihood of finding a ‘better’ dataset. There appears to be no significant and consistent difference in performance for visit sequences generated by different mechanisms.</p>

<p>A quick method for generating different visit sequences is to use the varrank package in R to plot feature importance for each dependent variable. This is significantly faster than using Random Forests to find feature importance. A heuristic is to place the most important variables at the front of the visit sequence.</p>

<h2 id="speeding-up-the-synthesis-process">Speeding Up the Synthesis Process</h2>

<ol>
  <li>
    <p>When experimenting with a new code, it is desirable to synthesise a small sub-sample of the dataset if the synthesis time is long. Use the synthpop::compare() function to do a preliminary examination to ensure that the code is working. Errors may only appear after the synthesis has proceeded for some time. Restarting R during synthesis will cause all local variables to be lost.</p>
  </li>
  <li>
    <p>For variables with many categories, one can combine less frequent categories as an ‘Others’ category. This reduces computational time during synthesis and is useful for removing outliers who face a higher disclosure risk. On the other hand, one may risk worsening general and specific utility.</p>
  </li>
  <li>
    <p>When working with (DateTime) formats, computational time is significantly improved if these are changed to numeric variables. One may also remove the days (and months) depending on the level of precision required. This improves computational time slightly.</p>
  </li>
  <li>
    <p>When working with factors, ensure that factor levels are coded as their original names(for instance as strings) rather than as integers. This seemingly trivial difference may cause hours of difference in synthesis time.</p>
  </li>
</ol>

<p>[1] Original document available at <a href="https://github.com/MUNFAI15/DiffPriv">https://github.com/MUNFAI15/DiffPriv</a></p>

<p>[2] Available for download in R using <a href="https://github.com/MUNFAI15/cmf">devtools::install(_)github(“MUNFAI15/cmf”)</a></p>
